{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, \"/root/AISMicroOrg\")\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from dataset_utils import build_reward_dataset, RewardDataCollatorWithPadding\n",
    "from reward_utils import compute_metrics, RewardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n",
    "    \"\"\"\n",
    "\n",
    "    local_rank: Optional[int] = field(\n",
    "        default=-1, metadata={\"help\": \"Used for multi-gpu\"}\n",
    "    )\n",
    "    resume_from_checkpoint: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"If you want to resume training where it left off.\"},\n",
    "    )\n",
    "    deepspeed: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU.\"\n",
    "        },\n",
    "    )\n",
    "    per_device_train_batch_size: Optional[int] = field(default=2)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=1)\n",
    "    learning_rate: Optional[float] = field(default=2e-5)\n",
    "    weight_decay: Optional[float] = field(default=0.001)\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"gpt2\",\n",
    "        metadata={\n",
    "            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n",
    "        },\n",
    "    )\n",
    "    data_folder: Optional[str] = field(\n",
    "        default=\"/root/AISMicroOrg/stack-exchange-paired_micro\",\n",
    "        metadata={\"help\": \"The path to the data folder.\"},\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The tokenizer for your model, if left empty will use the default for your model\",\n",
    "        },\n",
    "    )\n",
    "    bf16: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU.\"\n",
    "        },\n",
    "    )\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
    "    )\n",
    "    train_subset: Optional[int] = field(\n",
    "        default=20000,\n",
    "        metadata={\"help\": \"The size of the subset of the training data to use\"},\n",
    "    )\n",
    "    eval_subset: Optional[int] = field(\n",
    "        default=5000,\n",
    "        metadata={\"help\": \"The size of the subset of the eval data to use\"},\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"adamw_hf\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"linear\",\n",
    "        metadata={\"help\": \"The lr scheduler\"},\n",
    "    )\n",
    "    max_length: Optional[int] = field(default=512)\n",
    "    eval_first_step: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to run eval after the first step\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args = ScriptArguments()\n",
    "script_args.data_folder = \"/root/AISMicroOrg/stack-exchange-paired_micro\"\n",
    "script_args.model_name = \"/root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_split = script_args.model_name.split(\"/\")[-1]\n",
    "output_name = f\"{model_name_split}_peft_stack-exchange--paired_micro_rmts__{script_args.train_subset}_{script_args.learning_rate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    num_train_epochs=script_args.num_train_epochs,\n",
    "    weight_decay=script_args.weight_decay,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    deepspeed=script_args.deepspeed,\n",
    "    local_rank=script_args.local_rank,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    bf16=script_args.bf16,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    optim=script_args.optim,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    ")\n",
    "# Load the value-head model and tokenizer.\n",
    "tokenizer_name = (\n",
    "    script_args.tokenizer_name\n",
    "    if script_args.tokenizer_name is not None\n",
    "    else script_args.model_name\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493bca7afd36465f8a8c51f8da77d151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa02806c51c4ef7b6d4293591ae3cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, eval_dataset = build_reward_dataset(tokenizer, script_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Question: I am having trouble precisely telling the difference between a fact and an opinion. For example, let's say there is a man that weighs 500 pounds. Would it be a fact to refer to him as fat because that could proven. Another example would be saying Donald Trump is a bad president. Could that proven or would that just be an opinion?\\n\\nAnswer: The man weighs 500 pounds. That's a fact.\\n\\nThe man is fat. That's an opinion, although coming close to a common definition.\\n\\nFacts can be proved by experts in the respective field with (almost) always the same result. Facts always remain the same.\\n\\nOpinions can be differing. They can change from person to person or even from time to time. Here is an example: \\n\\nWhen I was young, I knew a lot of elder persons in Germany whom I considered fat. When I visited the USA for the first time, I saw a lot of persons whom I considered really as fat as I never had seen before. I remember a sheriff ordering a Coca Cola and pouring a pound or so of sugar into the drink. After that the people in Germany appeared no longer fat to me. (Alas meanwhile the Germans have caught up.)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0][\"input_ids_k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Question: I am having trouble precisely telling the difference between a fact and an opinion. For example, let\\'s say there is a man that weighs 500 pounds. Would it be a fact to refer to him as fat because that could proven. Another example would be saying Donald Trump is a bad president. Could that proven or would that just be an opinion?\\n\\nAnswer: I\\'d suggest that a proposition like your \"this man is fat\" can only be a \"fact\" if it\\'s operationally verifiable, as per, e.g., <https://plato.stanford.edu/entries/operationalism/> and <https://en.wikipedia.org/wiki/Verificationism>\\n\\nSo first you\\'d have to define \"fat\" as, say, \"ratio of weight-in-pounds divided by height-in-feet greater than 35\". And then you\\'d have to specify the construction of \"experimental apparatus\" like scales and rulers for your weight and height measurements.\\n\\nAnd now your statement, \"this man is fat\", is a true-or-false fact, at least with respect to your verifiable definition and measurement procedure. Of course, somebody else can come along and suggest that bmi (body mass index) is a better definition of \"fat\" to begin with, whereby your whole definition is just an opinion.\\n\\nBut now you simply need two different words, like \"bmi\\\\_fat\" and \"height/weight\\\\_fat\", to distinguish your meanings. And those words ultimately just refer back to your different bmi and height/weight measurement procedures. So a statement can only be a fact if it\\'s accompanied by an operational procedure that unambiguously determines whether it\\'s true or false.\\n\\nAnd I suppose you could try introducing \"good/bad\" definitions and measurements for Trump, too, but you\\'d probably run into people suggesting lots more alternatives than for \"fat\". And then it becomes a matter of opinion which definition/procedure to use. Until that\\'s specified, \"good/bad\"\\'s just an opinion; but once it\\'s specified, \"good/bad\"\\'s an operationally verifiable fact.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0][\"input_ids_j\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation :  8354\n",
      "finetune :  25152\n",
      "reward :  25704\n",
      "rl :  25488\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# from transformers import load_dataset\n",
    "\n",
    "dataset_name = \"/root/AISMicroOrg/stack-exchange-paired_micro/\"\n",
    "for n in [\"evaluation\", \"finetune\", \"reward\", \"rl\"]:\n",
    "    print(\n",
    "        n,\n",
    "        \": \",\n",
    "        load_dataset(dataset_name, data_dir=f\"data/{n}\", split=\"train\").num_rows,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
       "    num_rows: 25488\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickled_file(path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6315d5e577d34e56969c1b83fd75553d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,198,400 || all params: 6,611,546,112 || trainable%: 0.0635010318143267\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Need to do this for gpt2, because it doesn't have an official pad token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = not script_args.gradient_checkpointing\n",
    "num_proc = 24  # Can adjust to be higher if you have more processors.\n",
    "original_columns = train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "# Train the model, woohoo.\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, max_length=script_args.max_length\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomas-t\u001b[0m (\u001b[33mda-zealots\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/AISMicroOrg/reward_model/wandb/run-20231112_211444-3vhmbp4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da-zealots/huggingface/runs/3vhmbp4b' target=\"_blank\">chocolate-morning-28</a></strong> to <a href='https://wandb.ai/da-zealots/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da-zealots/huggingface' target=\"_blank\">https://wandb.ai/da-zealots/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da-zealots/huggingface/runs/3vhmbp4b' target=\"_blank\">https://wandb.ai/da-zealots/huggingface/runs/3vhmbp4b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3087' max='3087' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3087/3087 1:07:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.455100</td>\n",
       "      <td>1.275517</td>\n",
       "      <td>0.482716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.986979</td>\n",
       "      <td>0.562346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.770500</td>\n",
       "      <td>0.944661</td>\n",
       "      <td>0.594444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.025400</td>\n",
       "      <td>0.929540</td>\n",
       "      <td>0.623457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.398300</td>\n",
       "      <td>0.911660</td>\n",
       "      <td>0.625926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.953200</td>\n",
       "      <td>0.917703</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last checkpoint of the model\n"
     ]
    }
   ],
   "source": [
    "if script_args.eval_first_step:\n",
    "\n",
    "    class EvaluateFirstStepCallback(TrainerCallback):\n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            if state.global_step == 1:\n",
    "                control.should_evaluate = True\n",
    "\n",
    "    trainer.add_callback(EvaluateFirstStepCallback())\n",
    "\n",
    "trainer.train(script_args.resume_from_checkpoint)\n",
    "\n",
    "print(\"Saving last checkpoint of the model\")\n",
    "model.save_pretrained(output_name + \"_peft_last_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
